{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54263a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78454576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a09c01",
   "metadata": {},
   "source": [
    "__1968 - 1981__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d54ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b65595",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.dol.gov/agencies/whd/state/minimum-wage/history\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "tabelas = soup.find_all('table')\n",
    "\n",
    "df_wage = []\n",
    "for i, tabela in enumerate(tabelas):\n",
    "    linhas = tabela.find_all('tr')\n",
    "    cabecalho = linhas[0]\n",
    "    anos = [th.text for th in cabecalho.find_all('th')[1:]]\n",
    "    estados = []\n",
    "    for estado in linhas[1:]:\n",
    "        estados.append([td.text for td in estado.find_all('td')])\n",
    "    df = pd.DataFrame(estados, columns=['state'] + anos)\n",
    "    df_wage.append(df)\n",
    "df = pd.concat(df_wage, ignore_index=True)\n",
    "\n",
    "footnotes = soup.find('div', id='content')\n",
    "footnotes.find_all('p')\n",
    "list_footnotes = []\n",
    "for p in footnotes.find_all('p'):\n",
    "    if re.match(r'^[\\[\\(].[\\]\\)]', p.text):\n",
    "        id_footnote = p.text.strip().split(' ')[0]\n",
    "        text_footnote = ' '.join(p.text.strip().split(' ')[1:]).replace('- ','')\n",
    "        list_footnotes.append((id_footnote, text_footnote))\n",
    "footnotes_dict = {id_: text for id_, text in list_footnotes}\n",
    "columns_to_adjust = [col for col in df.columns if  not col.isnumeric() and col != 'Estado']\n",
    "footnote_year_bridge = {}\n",
    "for key, _ in footnotes_dict.items():\n",
    "    for col in columns_to_adjust:\n",
    "        if key in col:\n",
    "            footnote_year_bridge[col.replace(key, '').strip()] = key\n",
    "            df = df.rename(columns={col: col.replace(key, '').strip()})\n",
    "\n",
    "df_melted = df.melt(id_vars=['state'], var_name='year', value_name='minimal_wage').dropna()\n",
    "df_melted['year'] = df_melted['year'].astype(int)\n",
    "df_melted['minimal_wage'] = df_melted['minimal_wage'].str.replace('$', '')\n",
    "df_melted['id'] = df_melted.index + 1\n",
    "\n",
    "df_melted['minimal_wage'] = df_melted['minimal_wage'].str.replace(r'[\\[\\(].*?[\\]\\)]', '', regex=True)\n",
    "df_melted['minimal_wage'] = df_melted['minimal_wage'].mask(\n",
    "    df_melted['minimal_wage'].isin(['...', 'NA']), \n",
    "    pd.NA\n",
    ")\n",
    "if 'notes' not in df_melted.columns:\n",
    "    df_melted['notes'] = pd.NA\n",
    "\n",
    "if 'frequency' not in df_melted.columns:\n",
    "    df_melted['frequency'] = pd.NA\n",
    "\n",
    "\n",
    "def add_leading_zero(value):\n",
    "    value = value.strip()\n",
    "    if value.startswith('.'):\n",
    "        return '0' + value\n",
    "    return value\n",
    "\n",
    "# Fun√ß√£o para processar valores com m√∫ltiplas taxas\n",
    "def process_multiple_rates(row):\n",
    "    wage = row['minimal_wage']\n",
    "    \n",
    "    if pd.notna(wage) and isinstance(wage, str):\n",
    "        original_wage = wage\n",
    "        \n",
    "        # 1. Detectar e remover frequency markers\n",
    "        frequency = None\n",
    "        if '/day' in wage:\n",
    "            frequency = 2\n",
    "            wage = wage.replace('/day', '').strip()\n",
    "        elif '/wk' in wage:\n",
    "            frequency = 3\n",
    "            wage = wage.replace('/wk', '').strip()\n",
    "        \n",
    "        # 2. Detectar m√∫ltiplos valores com regex mais robusto\n",
    "        # Padr√£o: captura valores monet√°rios separados por -, &, /, ou espa√ßos\n",
    "        pattern = r'\\$?\\d+\\.?\\d*'\n",
    "        matches = re.findall(pattern, wage)\n",
    "        \n",
    "        if len(matches) >= 2:\n",
    "            first_value = add_leading_zero(matches[0])\n",
    "            second_value = add_leading_zero(matches[1])\n",
    "            \n",
    "            row['minimal_wage'] = first_value\n",
    "            note = f\"Or can be {second_value}, this reflects which rates differ by industry, occupation or other factors, as established under a wage-board type law\"\n",
    "            row['notes'] = note\n",
    "        elif len(matches) == 1:\n",
    "            row['minimal_wage'] = add_leading_zero(matches[0])\n",
    "        else:\n",
    "            row['minimal_wage'] = add_leading_zero(wage)\n",
    "        \n",
    "        # 3. Atualizar frequency\n",
    "        if frequency is not None:\n",
    "            row['frequency'] = frequency\n",
    "    \n",
    "    # Garantir valor padr√£o para frequency\n",
    "    if pd.isna(row['frequency']):\n",
    "        row['frequency'] = 1\n",
    "    \n",
    "    return row\n",
    "# Aplicar a fun√ß√£o\n",
    "df_melted = df_melted.apply(process_multiple_rates, axis=1)\n",
    "df_melted['minimal_wage'] = df_melted['minimal_wage'].astype(str).str.extract(r'([\\d.]+)', expand=False)\n",
    "\n",
    "def notes_for_null_wage(row):\n",
    "    if pd.isna(row['minimal_wage']) and pd.isna(row['notes']):\n",
    "        return \"This state utilizes the federal minimum wage\"\n",
    "    return row['notes']\n",
    "\n",
    "df_melted['notes'] = df_melted.apply(notes_for_null_wage, axis=1)  \n",
    "df_melted['minimal_wage'] = pd.to_numeric(df_melted['minimal_wage'], errors='coerce')\n",
    "df_final = df_melted[['id', 'state', 'year', 'minimal_wage', 'frequency','notes']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f6860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extrair_tabela_tipped_minimum_wage(year):\n",
    "    url = f'https://www.dol.gov/agencies/whd/state/minimum-wage/tipped/{year}'\n",
    "    tip_test = requests.get(url)\n",
    "    if tip_test.status_code != 200:\n",
    "        print(f\"‚ùå Falha ao obter dados de {year} (status {tip_test.status_code})\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    tip_soup = BeautifulSoup(tip_test.content, 'html.parser')\n",
    "\n",
    "    # 1. EXTRAIR FOOTNOTES\n",
    "    footnotes_dict = {}\n",
    "    for a_tag in tip_soup.find_all('a', attrs={'name': lambda x: x and x.startswith('foot')}):\n",
    "        name = a_tag.get('name')\n",
    "        parent_p = a_tag.find_parent('p')\n",
    "        if parent_p:\n",
    "            footnote_num = a_tag.get_text(strip=True)\n",
    "            texto_completo = ' '.join(parent_p.get_text().split())\n",
    "            texto_nota = texto_completo.replace(footnote_num, '', 1).strip()\n",
    "            footnotes_dict[name] = texto_nota\n",
    "\n",
    "    # 2. PROCESSAR TABELA\n",
    "    tip_table = tip_soup.find('table')\n",
    "    if not tip_table:\n",
    "        print(f\"‚ö†Ô∏è Nenhuma tabela encontrada em {year}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    tip_linhas = tip_table.find_all('tr')[1:]\n",
    "    header_order = ['jurisdiction', 'combinedrate', 'tipcredit', 'cashwage', 'definition']\n",
    "\n",
    "    def processar_celula_valor(td_element, column_name):\n",
    "        if not td_element:\n",
    "            return None, None, []\n",
    "        footnote_refs = []\n",
    "        for link in td_element.find_all('a', href=True):\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                match = re.search(r'#(foot\\d+)', href)\n",
    "                if match:\n",
    "                    footnote_refs.append(match.group(1))\n",
    "        td_html = str(td_element)\n",
    "        soup_copy = BeautifulSoup(td_html, 'html.parser')\n",
    "        for link in soup_copy.find_all('a'):\n",
    "            link.decompose()\n",
    "        valor = ' '.join(soup_copy.get_text().split())\n",
    "        footnote_texts = []\n",
    "        for ref in footnote_refs:\n",
    "            if ref in footnotes_dict:\n",
    "                footnote_texts.append(f\"[{column_name}] {footnotes_dict[ref]}\")\n",
    "        footnote_text = ' ; '.join(footnote_texts) if footnote_texts else None\n",
    "        return valor if valor else None, footnote_text, footnote_refs\n",
    "\n",
    "    def processar_jurisdiction(td_element):\n",
    "        if not td_element:\n",
    "            return None, None, None\n",
    "        footnote_refs = []\n",
    "        for link in td_element.find_all('a', href=True):\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                match = re.search(r'#(foot\\d+)', href)\n",
    "                if match:\n",
    "                    footnote_refs.append(match.group(1))\n",
    "        td_html = str(td_element)\n",
    "        soup_copy = BeautifulSoup(td_html, 'html.parser')\n",
    "        for link in soup_copy.find_all('a'):\n",
    "            link.decompose()\n",
    "        strong_tag = soup_copy.find('strong')\n",
    "        if strong_tag:\n",
    "            texto = ' '.join(strong_tag.get_text().split())\n",
    "            nome_limpo = re.sub(r'[^a-zA-Z0-9\\s]', '', texto)\n",
    "        else:\n",
    "            nome_limpo = soup_copy.get_text(strip=True)\n",
    "        extra_text = soup_copy.get_text().replace(nome_limpo, '').strip()\n",
    "        footnote_texts = [footnotes_dict[ref] for ref in footnote_refs if ref in footnotes_dict]\n",
    "        footnote_text = ' ; '.join(footnote_texts) if footnote_texts else None\n",
    "        return nome_limpo, footnote_text, extra_text\n",
    "\n",
    "    dados_tabela = []\n",
    "    ultima_jurisdiction = None\n",
    "    ultima_footnote = None\n",
    "    for tr in tip_linhas:\n",
    "        row_data = {}\n",
    "        tds = tr.find_all('td')\n",
    "        if tds and tds[0].get('colspan'):\n",
    "            continue\n",
    "        td_jurisdiction = tr.find('td', headers='jurisdiction')\n",
    "        todas_notas = []\n",
    "        if td_jurisdiction and td_jurisdiction.find('strong'):\n",
    "            jurisdiction_limpa, footnote_text, extra_text = processar_jurisdiction(td_jurisdiction)\n",
    "            ultima_jurisdiction = jurisdiction_limpa\n",
    "            ultima_footnote = footnote_text\n",
    "            row_data['jurisdiction'] = jurisdiction_limpa\n",
    "            if footnote_text:\n",
    "                todas_notas.append(footnote_text)\n",
    "            if extra_text:\n",
    "                todas_notas.append(extra_text)\n",
    "        else:\n",
    "            if ultima_jurisdiction:\n",
    "                row_data['jurisdiction'] = ultima_jurisdiction\n",
    "                if ultima_footnote:\n",
    "                    todas_notas.append(ultima_footnote)\n",
    "        for td in tds:\n",
    "            header_name = td.get('headers')[0] if td.get('headers') else None\n",
    "            if not header_name:\n",
    "                header_name = header_order[tds.index(td)] if len(tds) == 4 else header_order[tds.index(td) - 1]\n",
    "            valor_limpo, footnote_text, _ = processar_celula_valor(td, header_name)\n",
    "            if header_name != 'jurisdiction':\n",
    "                row_data[header_name] = valor_limpo\n",
    "            if footnote_text:\n",
    "                todas_notas.append(footnote_text)\n",
    "        if todas_notas:\n",
    "            row_data['notes'] = ' ; '.join(todas_notas)\n",
    "        if row_data and any(v for k, v in row_data.items() if k not in ['jurisdiction', 'notes']):\n",
    "            row_data['year'] = year\n",
    "            dados_tabela.append(row_data)\n",
    "\n",
    "    df_tips = pd.DataFrame(dados_tabela)\n",
    "    return df_tips\n",
    "\n",
    "\n",
    "# === LOOP PELOS ANOS 2003‚Äì2024 ===\n",
    "dfs = []\n",
    "for year in range(2024, 2025):\n",
    "    df_year = extrair_tabela_tipped_minimum_wage(year)\n",
    "    if not df_year.empty:\n",
    "        dfs.append(df_year)\n",
    "\n",
    "# Concatenar todos os DataFrames\n",
    "df_tips = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Mostrar resultado final\n",
    "print(f\"\\n‚úÖ Total de registros extra√≠dos: {len(df_tips)}\")\n",
    "\n",
    "def process_tip_wages(row):\n",
    "    \"\"\"\n",
    "    Processa valores de sal√°rio tipped, lidando com:\n",
    "    - M√∫ltiplos valores separados (-, &, /, espa√ßos)\n",
    "    - Textos descritivos movidos para notes\n",
    "    - Porcentagens e valores especiais\n",
    "    \"\"\"\n",
    "    \n",
    "    def is_monetary_value(value):\n",
    "        \"\"\"Verifica se √© um valor monet√°rio v√°lido\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return False\n",
    "        # Remove espa√ßos e verifica se tem formato de dinheiro\n",
    "        clean = value.strip()\n",
    "        # Padr√£o: pode ter $ e n√∫meros com ponto decimal\n",
    "        return bool(re.match(r'^\\$?\\d+\\.?\\d*$', clean))\n",
    "    \n",
    "    def is_percentage(value):\n",
    "        \"\"\"Verifica se √© uma porcentagem\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return False\n",
    "        return '%' in value or value.lower() in ['50%', 'to 50%']\n",
    "    \n",
    "    def extract_multiple_values(value):\n",
    "        \"\"\"Extrai m√∫ltiplos valores monet√°rios de uma string\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return None\n",
    "        \n",
    "        # Procurar por m√∫ltiplos valores monet√°rios\n",
    "        pattern = r'\\$?\\d+\\.?\\d*'\n",
    "        matches = re.findall(pattern, value)\n",
    "        \n",
    "        # Filtrar apenas valores que parecem dinheiro (com ou sem $)\n",
    "        valid_matches = [m for m in matches if re.match(r'^\\$?\\d+\\.\\d+$', m)]\n",
    "        \n",
    "        return valid_matches if len(valid_matches) > 1 else None\n",
    "    \n",
    "    def move_text_to_notes(column_name, value, row):\n",
    "        \"\"\"Move texto descritivo para notes\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return value, row\n",
    "        \n",
    "        # Se n√£o √© valor monet√°rio nem porcentagem, √© texto descritivo\n",
    "        if not is_monetary_value(value) and not is_percentage(value):\n",
    "            # Adicionar √† nota\n",
    "            note_text = f\"[{column_name}] {value}\"\n",
    "            \n",
    "            if pd.notna(row.get('notes')) and row['notes'] != 'Missing value':\n",
    "                row['notes'] = f\"{row['notes']} ; {note_text}\"\n",
    "            else:\n",
    "                row['notes'] = note_text\n",
    "            \n",
    "            return None, row  # Limpar o valor original\n",
    "        \n",
    "        return value, row\n",
    "    \n",
    "    # Processar cada coluna de valor\n",
    "    for col in ['combinedrate', 'tipcredit', 'cashwage']:\n",
    "        if col not in row:\n",
    "            continue\n",
    "            \n",
    "        value = row[col]\n",
    "        \n",
    "        if pd.isna(value) or value == 'Missing value':\n",
    "            continue\n",
    "        \n",
    "        # 1. Verificar se tem m√∫ltiplos valores\n",
    "        multiple_values = extract_multiple_values(value)\n",
    "        \n",
    "        if multiple_values:\n",
    "            # Tem m√∫ltiplos valores - usar o primeiro e criar nota\n",
    "            first_value = multiple_values[0]\n",
    "            if not first_value.startswith('$'):\n",
    "                first_value = f'${first_value}'\n",
    "            \n",
    "            row[col] = first_value\n",
    "            \n",
    "            # Criar nota com os valores alternativos\n",
    "            other_values = ', '.join(multiple_values[1:])\n",
    "            note_text = f\"[{col}] Alternative rate(s): {other_values}\"\n",
    "            \n",
    "            if pd.notna(row.get('notes')) and row['notes'] != 'Missing value':\n",
    "                row['notes'] = f\"{row['notes']} ; {note_text}\"\n",
    "            else:\n",
    "                row['notes'] = note_text\n",
    "        \n",
    "        # 2. Se n√£o √© valor monet√°rio nem porcentagem, mover para notes\n",
    "        else:\n",
    "            value, row = move_text_to_notes(col, value, row)\n",
    "            row[col] = value\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Aplicar a fun√ß√£o\n",
    "df_tips = df_tips.apply(process_tip_wages, axis=1)\n",
    "df_tips[['combinedrate', 'tipcredit', 'cashwage']] = df_tips[['combinedrate', 'tipcredit', 'cashwage']].apply(lambda x: x.str.replace('$', '', regex=False))\n",
    "\n",
    "def convert_with_context(value, column_name, row):\n",
    "    \"\"\"Converte e adiciona tipo na coluna + nota quando necess√°rio\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None, None, row\n",
    "    \n",
    "    if not isinstance(value, str):\n",
    "        return float(value) if isinstance(value, (int, float)) else None, 'exact', row\n",
    "    \n",
    "    original = value.strip()\n",
    "    value = original.replace('$', '')\n",
    "    \n",
    "    if value.lower() in ['not specified', 'missing value', '']:\n",
    "        return None, None, row\n",
    "    \n",
    "    # Porcentagem\n",
    "    if '%' in value:\n",
    "        match = re.search(r'(\\d+\\.?\\d*)\\s*%', value)\n",
    "        if match:\n",
    "            note = f\"[{column_name}] Original value: {original}\"\n",
    "            if pd.notna(row.get('notes')) and row['notes'] != 'Missing value':\n",
    "                row['notes'] = f\"{row['notes']} ; {note}\"\n",
    "            else:\n",
    "                row['notes'] = note\n",
    "            return float(match.group(1)), 'percentage', row\n",
    "    \n",
    "    # Range\n",
    "    range_patterns = {\n",
    "        'up to': r'up to\\s+(\\d+\\.?\\d*)',\n",
    "        'more than': r'more than\\s+(\\d+\\.?\\d*)',\n",
    "        'at least': r'at least\\s+(\\d+\\.?\\d*)'\n",
    "    }\n",
    "    \n",
    "    for range_type, pattern in range_patterns.items():\n",
    "        match = re.search(pattern, value, re.IGNORECASE)\n",
    "        if match:\n",
    "            note = f\"[{column_name}] {range_type.capitalize()} {match.group(1)}\"\n",
    "            if pd.notna(row.get('notes')) and row['notes'] != 'Missing value':\n",
    "                row['notes'] = f\"{row['notes']} ; {note}\"\n",
    "            else:\n",
    "                row['notes'] = note\n",
    "            return float(match.group(1)), 'range', row\n",
    "    \n",
    "    # Exato\n",
    "    try:\n",
    "        return float(value), 'exact', row\n",
    "    except ValueError:\n",
    "        return None, None, row\n",
    "\n",
    "# Aplicar\n",
    "def process_with_types(row):\n",
    "    for col in ['combinedrate', 'tipcredit', 'cashwage']:\n",
    "        if col in row:\n",
    "            value, value_type, row = convert_with_context(row[col], col, row)\n",
    "            row[col] = value\n",
    "            row[f'{col}_type'] = value_type\n",
    "    return row\n",
    "\n",
    "df_tips = df_tips.apply(process_with_types, axis=1)\n",
    "df_tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "URL = \"https://www.dol.gov/agencies/whd/state/age-certificates\"\n",
    "\n",
    "response = requests.get(URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "if not table:\n",
    "    raise ValueError(\"Tabela n√£o encontrada na p√°gina.\")\n",
    "\n",
    "rows = table.find_all(\"tr\")[4:]\n",
    "\n",
    "\n",
    "def detect_requirement_level(text: str):\n",
    "    \"\"\"Identifica o(s) n√≠vel(is) de requisito no texto.\"\"\"\n",
    "    mapping = {'(M)': 1, '(R)': 2, '(P)': 3}\n",
    "    return [level for mark, level in mapping.items() if mark in text]\n",
    "\n",
    "\n",
    "def extract_text(td):\n",
    "    \"\"\"Extrai texto limpo de uma c√©lula <td>.\"\"\"\n",
    "    return '; '.join(part.strip() for part in td.stripped_strings)\n",
    "\n",
    "def remove_requirement_marks(text: str):\n",
    "    \"\"\"Remove marcas de requisito do texto.\"\"\"\n",
    "    return re.sub(r'\\s*\\(M\\)|\\s*\\(R\\)|\\s*\\(P\\)', '', text)\n",
    "\n",
    "def detect_footnote(values):\n",
    "    \"\"\"Remove links das c√©lulas e retorna as refer√™ncias encontradas.\"\"\"\n",
    "    links = []\n",
    "    for idx, td in enumerate(values):\n",
    "        anchors = td.find_all(\"a\", href=True)\n",
    "        if anchors:\n",
    "            for link in anchors:\n",
    "                href = link.text.strip()\n",
    "                link.decompose()\n",
    "                links.append({\n",
    "                    \"href\": href,\n",
    "                    \"index\": idx,\n",
    "                    \"clean_td\": td\n",
    "                })\n",
    "    return links or None\n",
    "\n",
    "\n",
    "def parse_state_row(state_row):\n",
    "    \"\"\"Extrai informa√ß√µes estruturadas de uma linha da tabela.\"\"\"\n",
    "    jurisdiction = state_row.th.strong.get_text(strip=True)\n",
    "    values = state_row.find_all(\"td\")\n",
    "    \n",
    "    if len(values) < 6:\n",
    "        return None\n",
    "\n",
    "    clean_texts = detect_footnote(values)\n",
    "    if clean_texts:\n",
    "        for ref in clean_texts:\n",
    "            values[ref[\"index\"]] = ref[\"clean_td\"]\n",
    "\n",
    "    v = [extract_text(td) for td in values]\n",
    "\n",
    "    employment = {\n",
    "        \"state\": jurisdiction,\n",
    "        \"certificate_type\": \"employment\",\n",
    "        \"rule_description\": remove_requirement_marks(v[0]),\n",
    "        \"is_labor\": \"1\" if \"X\" in v[1] else \"0\",\n",
    "        \"is_school\": \"1\" if \"X\" in v[2] else \"0\",\n",
    "        \"requirement_level\": detect_requirement_level(v[0]),\n",
    "        \"notes\": f\"Labor: {v[1].replace('X', '').strip() or 'N√£o'}; School: {v[2].replace('X', '').strip() or 'N√£o'}\",\n",
    "        \"footnotes\": [clean_texts[i]['href'] for i in range(len(clean_texts)) if clean_texts[i]['index'] <= 2] if clean_texts else None\n",
    "    }\n",
    "\n",
    "    age = {\n",
    "        \"state\": jurisdiction,\n",
    "        \"certificate_type\": \"age\",\n",
    "        \"rule_description\": remove_requirement_marks(v[3]),\n",
    "        \"is_labor\": \"1\" if \"X\" in v[4] else \"0\",\n",
    "        \"is_school\": \"1\" if \"X\" in v[5] else \"0\",\n",
    "        \"requirement_level\": detect_requirement_level(v[3]),\n",
    "        \"notes\": f\"Labor: {v[4].replace('X', '').strip() or 'N√£o'}; School: {v[5].replace('X', '').strip() or 'N√£o'}\",\n",
    "        \"footnotes\": [clean_texts[i]['href'] for i in range(len(clean_texts)) if clean_texts[i]['index'] >= 3] if clean_texts else None\n",
    "    }\n",
    "    return employment, age\n",
    "\n",
    "\n",
    "youth_employment = []\n",
    "\n",
    "for row in rows:\n",
    "    parsed = parse_state_row(row)\n",
    "    if parsed:\n",
    "        youth_employment.extend(parsed)\n",
    "df_youth_employment = pd.DataFrame(youth_employment)\n",
    "df_youth_employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scraper para dados de tipped minimum wage\n",
    "\"\"\"\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List, Tuple\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import BASE_URL_TIPPED_WAGE, TIPPED_WAGE_START_YEAR, TIPPED_WAGE_END_YEAR, REQUEST_TIMEOUT\n",
    "\n",
    "\n",
    "class TippedWageScraper:\n",
    "    \"\"\"Classe para extrair dados de tipped minimum wage\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = BASE_URL_TIPPED_WAGE):\n",
    "        self.base_url = base_url\n",
    "        self.header_order = ['jurisdiction', 'combinedrate', 'tipcredit', 'cashwage', 'definition']\n",
    "        self.footnotes_dict = {}\n",
    "    \n",
    "    def extract_footnotes(self, soup: BeautifulSoup) -> Dict[str, str]:\n",
    "        \"\"\"Extrai footnotes de uma p√°gina\"\"\"\n",
    "        footnotes_dict = {}\n",
    "        \n",
    "        for a_tag in soup.find_all('a', attrs={'name': lambda x: x and x.startswith('foot')}):\n",
    "            name = a_tag.get('name')\n",
    "            parent_p = a_tag.find_parent('p')\n",
    "            \n",
    "            if parent_p:\n",
    "                footnote_num = a_tag.get_text(strip=True)\n",
    "                texto_completo = ' '.join(parent_p.get_text().split())\n",
    "                texto_nota = texto_completo.replace(footnote_num, '', 1).strip()\n",
    "                footnotes_dict[name] = texto_nota\n",
    "        \n",
    "        return footnotes_dict\n",
    "    \n",
    "    def processar_celula_valor(self, td_element, column_name: str, footnotes_dict: Dict) -> tuple:\n",
    "        \"\"\"Extrai valor limpo e footnotes de uma c√©lula\"\"\"\n",
    "        if not td_element:\n",
    "            return None, [], []\n",
    "        \n",
    "        # Procurar links de footnote\n",
    "        footnote_refs = []\n",
    "        for link in td_element.find_all('a', href=True):\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                match = re.search(r'#(foot\\d+)', href)\n",
    "                if match:\n",
    "                    footnote_refs.append(match.group(1))\n",
    "        \n",
    "        # Criar c√≥pia e remover links\n",
    "        td_html = str(td_element)\n",
    "        soup_copy = BeautifulSoup(td_html, 'html.parser')\n",
    "        \n",
    "        for link in soup_copy.find_all('a'):\n",
    "            link.decompose()\n",
    "        # Extrair valor limpo\n",
    "        valor = ' '.join(soup_copy.get_text().split())\n",
    "        if soup_copy.find('strong'):\n",
    "            valor = None\n",
    "        return valor if valor else None, footnote_refs\n",
    "\n",
    "    def processar_jurisdiction(self, td_element, footnotes_dict: Dict) -> Tuple[str, list, str]:\n",
    "        \"\"\"Extrai o nome limpo da jurisdiction e seus footnotes\"\"\"\n",
    "        if not td_element:\n",
    "            return None, [], \"\"\n",
    "        \n",
    "        # Extrair footnotes a partir dos links\n",
    "        footnote_refs = []\n",
    "        for link in td_element.find_all('a', href=True):\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                match = re.search(r'#(foot\\d+)', href)\n",
    "                if match:\n",
    "                    footnote_refs.append(match.group(1))\n",
    "\n",
    "        # Copiar HTML e remover os links e strongs para n√£o afetar o texto final\n",
    "        soup_copy = BeautifulSoup(str(td_element), 'html.parser')\n",
    "\n",
    "        # Extrair nome da jurisdi√ß√£o (normalmente no primeiro <strong>)\n",
    "        first_strong = soup_copy.find('strong')\n",
    "        if first_strong:\n",
    "            nome_limpo = re.sub(r'[^a-zA-Z0-9\\s]', '', first_strong.get_text(strip=True))\n",
    "        else:\n",
    "            nome_limpo = soup_copy.get_text(strip=True)\n",
    "        \n",
    "\n",
    "        # Remover <strong> e <a> completamente para isolar o texto explicativo\n",
    "        for tag in soup_copy.find_all(['strong', 'a']):\n",
    "            tag.decompose()\n",
    "        if nome_limpo == 'Ohio':\n",
    "            print(f\"Jurisdiction raw: {td_element}\")\n",
    "            print(f\"Jurisdiction processed: {soup_copy}\")\n",
    "            print(f\"Other text: {' '.join(soup_copy.get_text(strip=True).split())}\")\n",
    "        # Agora pegar s√≥ o texto restante\n",
    "        other_extra_text = ' '.join(soup_copy.get_text(strip=True).split())\n",
    "\n",
    "        return nome_limpo, footnote_refs, other_extra_text\n",
    "\n",
    "\n",
    "\n",
    "    def extract_table_for_year(self, year: int) -> pd.DataFrame:\n",
    "        \"\"\"Extrai tabela de um ano espec√≠fico\"\"\"\n",
    "        url = f'{self.base_url}/{year}'\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=REQUEST_TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extrair footnotes\n",
    "        footnotes = self.extract_footnotes(soup)\n",
    "        print(footnotes)\n",
    "        self.footnotes_dict[year] = footnotes\n",
    "        # Processar tabela\n",
    "        tip_table = soup.find('table')\n",
    "        if not tip_table:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        tip_linhas = tip_table.find_all('tr')[1:]\n",
    "        \n",
    "        dados_tabela = []\n",
    "        ultima_jurisdiction = None\n",
    "        ultima_footnote = None\n",
    "        ultima_footnote_refs = []\n",
    "        \n",
    "        for tr in tip_linhas:\n",
    "            row_data = {}\n",
    "            tds = tr.find_all('td')\n",
    "            if len(tds) > len(self.header_order):\n",
    "                tds.pop(1)\n",
    "            # Pular linhas com colspan\n",
    "            if tds and tds[0].get('colspan'):\n",
    "                continue\n",
    "            \n",
    "            td_jurisdiction = tr.find('td', headers='jurisdiction')\n",
    "            todas_notas = []\n",
    "            all_footnote_refs = []\n",
    "            all_footnote_texts = []\n",
    "            \n",
    "            if td_jurisdiction and td_jurisdiction.find('strong'):\n",
    "                jurisdiction_limpa, footnote_refs, note_text = self.processar_jurisdiction(\n",
    "                    td_jurisdiction, self.footnotes_dict\n",
    "                )\n",
    "                if jurisdiction_limpa == 'Ohio':\n",
    "                    print(f\"Jurisdiction raw: {td_jurisdiction}\")\n",
    "                    print(f\"Jurisdiction processed: {jurisdiction_limpa}\")\n",
    "                    print(f\"Footnote refs: {footnote_refs}\")\n",
    "                    print(f\"Note text: {note_text}\")\n",
    "                ultima_jurisdiction = jurisdiction_limpa\n",
    "                ultima_footnote_refs = footnote_refs\n",
    "                if note_text:\n",
    "                    if jurisdiction_limpa == 'Ohio':\n",
    "                        print(f\"Note text for Ohio: {note_text}\")\n",
    "                    row_data['notes'] = note_text\n",
    "                row_data['jurisdiction'] = jurisdiction_limpa\n",
    "                \n",
    "                if footnote_refs:\n",
    "                    all_footnote_refs.extend(footnote_refs)\n",
    "            else:\n",
    "                if ultima_jurisdiction:\n",
    "                    row_data['jurisdiction'] = ultima_jurisdiction\n",
    "                    if ultima_footnote_refs:\n",
    "                        all_footnote_refs.extend(ultima_footnote_refs)\n",
    "            \n",
    "            # Processar valores das colunas\n",
    "            for i, td in enumerate(tds):\n",
    "                header_name = td.get('headers')[0] if td.get('headers') else None\n",
    "                if not header_name:\n",
    "                    header_name = self.header_order[i]\n",
    "  \n",
    "                \n",
    "                valor_limpo, footnote_refs = self.processar_celula_valor(\n",
    "                    td, header_name, self.footnotes_dict\n",
    "                )\n",
    "                if valor_limpo:\n",
    "                    if header_name != 'jurisdiction':\n",
    "                        row_data[header_name] = valor_limpo\n",
    "                    else:\n",
    "                        row_data['notes'] = valor_limpo\n",
    "                    if footnote_refs:\n",
    "                        all_footnote_refs.extend(footnote_refs)\n",
    "                \n",
    "            \n",
    "            if all_footnote_refs:\n",
    "                row_data['footnotes'] = list(set(all_footnote_refs))  # Remove duplicatas\n",
    "            \n",
    "            if row_data and any(v for k, v in row_data.items() if k not in ['jurisdiction', 'notes', 'footnotes']):\n",
    "                row_data['year'] = year\n",
    "                dados_tabela.append(row_data)\n",
    "\n",
    "        return pd.DataFrame(dados_tabela)\n",
    "    \n",
    "    def scrape(self, start_year: int = TIPPED_WAGE_START_YEAR, \n",
    "               end_year: int = TIPPED_WAGE_END_YEAR) -> pd.DataFrame:\n",
    "        \"\"\"Executa o scraping para todos os anos\"\"\"\n",
    "        \n",
    "        dfs = []\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            df_year = self.extract_table_for_year(year)\n",
    "            if not df_year.empty:\n",
    "                dfs.append(df_year)\n",
    "                print(f\"‚úì {len(df_year)} registros\")\n",
    "            else:\n",
    "                print(\"‚úó\")\n",
    "        \n",
    "        if not dfs:\n",
    "            print(\"‚ùå Nenhum dado foi extra√≠do\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df_final = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        return df_final\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fun√ß√£o principal para teste\"\"\"\n",
    "    scraper = TippedWageScraper()\n",
    "    df = scraper.scrape(start_year=2003, end_year=2003)  # Teste com poucos anos\n",
    "    print(\"\\nüìã Preview dos dados:\")\n",
    "    print(scraper.footnotes_dict)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ec37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processador de dados de tipped minimum wage\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import is_monetary_value, is_percentage, extract_multiple_values, append_note, consolidate_notes_simple \n",
    "\n",
    "class TippedWageProcessor:\n",
    "    \"\"\"Classe para processar dados de tipped minimum wage\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "    \n",
    "    def move_text_to_notes(self, column_name: str, value, row):\n",
    "        \"\"\"Move texto descritivo para notes\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return value, row\n",
    "        \n",
    "        # Se n√£o √© valor monet√°rio nem porcentagem, √© texto descritivo\n",
    "        if not is_monetary_value(value) and not is_percentage(value):\n",
    "            note_text = f\"[{column_name}] {value}\"\n",
    "            row['notes'] = append_note(row.get('notes'), note_text)\n",
    "            return None, row\n",
    "        \n",
    "        return value, row\n",
    "    \n",
    "    def process_tip_wages(self, row):\n",
    "        \"\"\"Processa valores de sal√°rio tipped\"\"\"\n",
    "        for col in ['combinedrate', 'tipcredit', 'cashwage']:\n",
    "            if col not in row:\n",
    "                continue\n",
    "            \n",
    "            value = row[col]\n",
    "            \n",
    "            if pd.isna(value) or value == 'Missing value':\n",
    "                continue\n",
    "            \n",
    "            # 1. Verificar se tem m√∫ltiplos valores\n",
    "            multiple_values = extract_multiple_values(value)\n",
    "            \n",
    "            if multiple_values:\n",
    "                first_value = multiple_values[0]\n",
    "                if not first_value.startswith('$'):\n",
    "                    first_value = f'${first_value}'\n",
    "                \n",
    "                row[col] = first_value\n",
    "                \n",
    "                other_values = ', '.join(multiple_values[1:])\n",
    "                note_text = f\"[{col}] Alternative rate(s): {other_values}\"\n",
    "                row['notes'] = append_note(row.get('notes'), note_text)\n",
    "            \n",
    "            # 2. Se n√£o √© valor monet√°rio nem porcentagem, mover para notes\n",
    "            else:\n",
    "                value, row = self.move_text_to_notes(col, value, row)\n",
    "                row[col] = value\n",
    "        \n",
    "        return row\n",
    "    \n",
    "    def convert_with_context(self, value, column_name: str, row):\n",
    "        \"\"\"Converte valores para float mantendo contexto\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return None, None, row\n",
    "        \n",
    "        if not isinstance(value, str):\n",
    "            return float(value) if isinstance(value, (int, float)) else None, 'exact', row\n",
    "        \n",
    "        original = value.strip()\n",
    "        value = original.replace('$', '')\n",
    "        \n",
    "        if value.lower() in ['not specified', 'missing value', '']:\n",
    "            return None, None, row\n",
    "        \n",
    "        # Porcentagem\n",
    "        if '%' in value:\n",
    "            match = re.search(r'(\\d+\\.?\\d*)\\s*%', value)\n",
    "            if match:\n",
    "                note = f\"[{column_name}] Original value: {original}\"\n",
    "                row['notes'] = append_note(row.get('notes'), note)\n",
    "                return float(match.group(1)), 'percentage', row\n",
    "        \n",
    "        # Range (up to, more than, at least)\n",
    "        range_patterns = {\n",
    "            'up to': r'up to\\s+(\\d+\\.?\\d*)',\n",
    "            'more than': r'more than\\s+(\\d+\\.?\\d*)',\n",
    "            'at least': r'at least\\s+(\\d+\\.?\\d*)',\n",
    "            'to': r'to\\s+(\\d+\\.?\\d*)'\n",
    "        }\n",
    "        \n",
    "        for range_type, pattern in range_patterns.items():\n",
    "            match = re.search(pattern, value, re.IGNORECASE)\n",
    "            if match:\n",
    "                note = f\"[{column_name}] {range_type.capitalize()} {match.group(1)}\"\n",
    "                row['notes'] = append_note(row.get('notes'), note)\n",
    "                return float(match.group(1)), 'range', row\n",
    "        \n",
    "        # Valor exato\n",
    "        try:\n",
    "            return float(value), 'exact', row\n",
    "        except ValueError:\n",
    "            return None, None, row\n",
    "    \n",
    "    def process_with_types(self, row):\n",
    "        \"\"\"Processa valores e adiciona tipo\"\"\"\n",
    "        for col in ['combinedrate', 'tipcredit', 'cashwage']:\n",
    "            if col in row:\n",
    "                value, value_type, row = self.convert_with_context(row[col], col, row)\n",
    "                row[col] = value\n",
    "                row[f'{col}_type'] = value_type\n",
    "        return row\n",
    "    \n",
    "    def process(self) -> pd.DataFrame:\n",
    "        \"\"\"Executa o pipeline completo de processamento\"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        \n",
    "        df = df.apply(self.process_tip_wages, axis=1)\n",
    "        for col in ['combinedrate', 'tipcredit', 'cashwage']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(lambda x: x.str.replace('$', '', regex=False) if hasattr(x, 'str') else x)\n",
    "        df = df.apply(self.process_with_types, axis=1)\n",
    "        df['notes'] = df.apply(lambda row: consolidate_notes_simple(row['notes'], row['definition']), axis=1)\n",
    "        return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fun√ß√£o principal para teste\"\"\"\n",
    "    # Criar dados de exemplo\n",
    "    objects = TippedWageScraper() \n",
    "    df = objects.scrape()\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "    processor = TippedWageProcessor(df)\n",
    "    df_processed = processor.process()\n",
    "    display(df_processed)\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e518642",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StandardWageProcessor' from 'processors.processor_standard_wage' (/home/agermano/projeto_adp/jupyter/processors/processor_standard_wage.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscrapers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscrapper_minimum_wage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MinimumWageScraper\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscrapers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscrapper_tipped_wage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TippedWageScraper\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprocessors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessor_standard_wage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardWageProcessor\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprocessors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessor_tipped_wage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TippedWageProcessor\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDataTransformer\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'StandardWageProcessor' from 'processors.processor_standard_wage' (/home/agermano/projeto_adp/jupyter/processors/processor_standard_wage.py)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transformador que unifica os datasets de sal√°rio padr√£o e tipped wages\n",
    "Cria a estrutura dimensional (Star Schema) com Dim_Footnotes normalizado\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import generate_hash\n",
    "from config import WAGE_CATEGORIES, TIPPED_WAGE_TYPE\n",
    "import re\n",
    "from scrapers.scrapper_minimum_wage import MinimumWageScraper\n",
    "from scrapers.scrapper_tipped_wage import TippedWageScraper\n",
    "from processors.processor_standard_wage import StandardWageProcessor\n",
    "from processors.processor_tipped_wage import TippedWageProcessor\n",
    "\n",
    "class DataTransformer:\n",
    "    \"\"\"Classe para transformar e unificar os datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, df_standard: pd.DataFrame, df_tipped: pd.DataFrame):\n",
    "        self.df_standard = df_standard\n",
    "        self.df_tipped = df_tipped\n",
    "        \n",
    "        # Tabelas dimensionais\n",
    "        self.dim_states = None\n",
    "        self.dim_categories = None\n",
    "        self.dim_footnotes = None\n",
    "        self.fact_minimum_wage = None\n",
    "        self.bridge_wage_footnote = None\n",
    "\n",
    "    \n",
    "    def create_dim_footnotes_unified(self, all_footnotes_dict: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cria Dim_Footnotes unificada de todos os datasets\n",
    "        \n",
    "        Args:\n",
    "            all_footnotes_dict: {footnote_key: footnote_text}\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame com footnote_id, footnote_key, footnote_text\n",
    "        \"\"\"\n",
    "        \n",
    "        footnotes_data = []\n",
    "        for key, text in all_footnotes_dict.items():\n",
    "            for foot_key, foot_text in text.items():\n",
    "                footnotes_data.append({\n",
    "                    'footnote_key': foot_key,\n",
    "                    'footnote_text': foot_text,\n",
    "                    'footnote_year': int(key) if key != 1 else None,\n",
    "                    'category_id': 1 if key == 1 else 2,\n",
    "                    'footnote_hash': generate_hash(foot_text)\n",
    "                })\n",
    "        \n",
    "        df_footnotes = pd.DataFrame(footnotes_data)\n",
    "        df_footnotes = df_footnotes.drop_duplicates(subset=['footnote_key','footnote_hash'])\n",
    "        \n",
    "        df_footnotes['footnote_id'] = range(1, len(df_footnotes) + 1)\n",
    "        \n",
    "        self.dim_footnotes = df_footnotes[['footnote_id', 'footnote_key', 'footnote_text', 'footnote_year', 'category_id']]\n",
    "        \n",
    "        return self.dim_footnotes\n",
    "    \n",
    "    def transform_standard_to_long(self) -> pd.DataFrame:\n",
    "        \"\"\"Transforma dataset padr√£o para formato compat√≠vel\"\"\"        \n",
    "        df = self.df_standard.copy()\n",
    "        df = df.rename(columns={'state': 'jurisdiction', 'minimal_wage': 'base_wage_per_hour'})\n",
    "        \n",
    "        df['category_name'] = WAGE_CATEGORIES['standard']\n",
    "        df['category_type'] = 'standard'\n",
    "        df['minimum_cash_wage'] = None\n",
    "        df['maximum_tip_credit'] = None\n",
    "        df['source_url'] = 'https://www.dol.gov/agencies/whd/state/minimum-wage/history'\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform_tipped_to_long(self) -> pd.DataFrame:\n",
    "        \"\"\"Transforma dataset tipped para formato long\"\"\"        \n",
    "        df_long = []\n",
    "        \n",
    "        for _, row in self.df_tipped.iterrows():\n",
    "            # Notes e footnotes j√° v√™m separados do scraper\n",
    "            notes_clean = row.get('notes')    \n",
    "                    \n",
    "            base_row = {\n",
    "                'jurisdiction': row['jurisdiction'],\n",
    "                'year': row['year'],\n",
    "                'frequency': 1,\n",
    "                'source_url': f\"https://www.dol.gov/agencies/whd/state/minimum-wage/tipped/{row['year']}\",\n",
    "                'notes': notes_clean\n",
    "            }\n",
    "            \n",
    "            # Combined Rate\n",
    "            if pd.notna(row.get('combinedrate')):\n",
    "                df_long.append({\n",
    "                    **base_row,\n",
    "                    'category_name': WAGE_CATEGORIES['tipped_combined'],\n",
    "                    'category_type': 'tipped',\n",
    "                    'base_wage_per_hour': row['combinedrate'],\n",
    "                    'value_type': TIPPED_WAGE_TYPE[row.get('combinedrate_type')],\n",
    "                    'minimum_cash_wage': None,\n",
    "                    'maximum_tip_credit': None\n",
    "                })\n",
    "            \n",
    "            # Tip Credit\n",
    "            if pd.notna(row.get('tipcredit')):\n",
    "                df_long.append({\n",
    "                    **base_row,\n",
    "                    'category_name': WAGE_CATEGORIES['tipped_credit'],\n",
    "                    'category_type': 'tipped',\n",
    "                    'base_wage_per_hour': None,\n",
    "                    'value_type': TIPPED_WAGE_TYPE[row.get('tipcredit_type')],\n",
    "                    'minimum_cash_wage': None,\n",
    "                    'maximum_tip_credit': row['tipcredit']\n",
    "                })\n",
    "            \n",
    "            # Cash Wage\n",
    "            if pd.notna(row.get('cashwage')):\n",
    "                df_long.append({\n",
    "                    **base_row,\n",
    "                    'category_name': WAGE_CATEGORIES['tipped_cash'],\n",
    "                    'category_type': 'tipped',\n",
    "                    'base_wage_per_hour': None,\n",
    "                    'value_type': TIPPED_WAGE_TYPE[row.get('cashwage_type')],\n",
    "                    'minimum_cash_wage': row['cashwage'],\n",
    "                    'maximum_tip_credit': None\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(df_long)\n",
    "    \n",
    "    def create_dim_states(self, df_unified: pd.DataFrame):\n",
    "        \n",
    "        states = df_unified[['jurisdiction']].drop_duplicates().reset_index(drop=True)\n",
    "        states = states.rename(columns={'jurisdiction': 'state_name'})\n",
    "        states['state_id'] = states.index + 1\n",
    "        states['is_territory'] = False\n",
    "        \n",
    "        self.dim_states = states[['state_id', 'state_name', 'is_territory']]\n",
    "        return self.dim_states\n",
    "    \n",
    "    def create_dim_categories(self, df_unified: pd.DataFrame):\n",
    "        \n",
    "        categories = df_unified[['category_name', 'category_type']].drop_duplicates().reset_index(drop=True)\n",
    "        categories['category_id'] = categories.index + 1\n",
    "        \n",
    "        self.dim_categories = categories[['category_id', 'category_name', 'category_type']]\n",
    "        return self.dim_categories\n",
    "    \n",
    "    def create_fact_table(self, df_unified: pd.DataFrame):\n",
    "        display(df_unified)\n",
    "        # Merge com dimens√µes\n",
    "        df = df_unified.merge(\n",
    "            self.dim_states.rename(columns={'state_name': 'jurisdiction'}),\n",
    "            on='jurisdiction',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        df = df.merge(\n",
    "            self.dim_categories[['category_id', 'category_name']],\n",
    "            on='category_name',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Guardar footnotes para criar bridge depois\n",
    "        self.footnote_refs_by_wage = df[['footnotes']].copy()\n",
    "        self.footnote_refs_by_wage.index = range(1, len(self.footnote_refs_by_wage) + 1)\n",
    "        self.footnote_refs_by_wage['wage_id'] = self.footnote_refs_by_wage.index\n",
    "        \n",
    "        # Selecionar colunas da fato (SEM footnotes)\n",
    "        fact_columns = [\n",
    "            'state_id', 'category_id', 'year', 'base_wage_per_hour',\n",
    "            'minimum_cash_wage', 'maximum_tip_credit', 'frequency', 'source_url', 'notes'\n",
    "        ]\n",
    "        \n",
    "        fact = df[[c for c in fact_columns if c in df.columns]].copy()\n",
    "        fact['wage_id'] = range(1, len(fact) + 1)\n",
    "        fact['effective_date'] = None\n",
    "        \n",
    "        # Reordenar (notes fica, footnotes N√ÉO)\n",
    "        self.fact_minimum_wage = fact[[\n",
    "            'wage_id', 'state_id', 'category_id', 'year', 'effective_date',\n",
    "            'base_wage_per_hour', 'minimum_cash_wage', 'maximum_tip_credit',\n",
    "            'frequency', 'source_url', 'notes'\n",
    "        ]]\n",
    "        \n",
    "        return self.fact_minimum_wage\n",
    "    \n",
    "    def create_bridge_table(self, all_footnotes_dict: dict):\n",
    "        \"\"\"Cria tabela bridge entre wage e footnotes\"\"\"\n",
    "        \n",
    "        bridge_data = []\n",
    "        \n",
    "        for _, row in self.footnote_refs_by_wage.iterrows():\n",
    "            wage_id = row['wage_id']\n",
    "            footnote_refs = row.get('footnotes')\n",
    "            \n",
    "            if pd.isna(footnote_refs):\n",
    "                continue\n",
    "            \n",
    "            # footnote_refs pode ser lista ou string\n",
    "            if isinstance(footnote_refs, str):\n",
    "                footnote_refs = [footnote_refs]\n",
    "            elif not isinstance(footnote_refs, list):\n",
    "                continue\n",
    "            \n",
    "            for ref in footnote_refs:\n",
    "                # Buscar footnote_id correspondente no Dim_Footnotes\n",
    "                footnote_row = self.dim_footnotes[self.dim_footnotes['footnote_key'] == ref]\n",
    "                \n",
    "                if not footnote_row.empty:\n",
    "                    bridge_data.append({\n",
    "                        'wage_id': wage_id,\n",
    "                        'footnote_id': footnote_row.iloc[0]['footnote_id']\n",
    "                    })\n",
    "        \n",
    "        if bridge_data:\n",
    "            self.bridge_wage_footnote = pd.DataFrame(bridge_data).drop_duplicates()\n",
    "        else:\n",
    "            self.bridge_wage_footnote = pd.DataFrame(columns=['wage_id', 'footnote_id'])\n",
    "        \n",
    "        return self.bridge_wage_footnote\n",
    "    \n",
    "    def default_state_name(self, df_unified):\n",
    "        df_unified['jurisdiction'] = df_unified['jurisdiction'].str.replace('\\t','')\n",
    "        df_unified['jurisdiction'] = df_unified['jurisdiction'].str.replace('\\n',' ')\n",
    "        df_unified['jurisdiction'] = df_unified['jurisdiction'].apply(lambda x: ' '.join(x.split()))\n",
    "        df_unified['jurisdiction'] = df_unified['jurisdiction'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '',x))\n",
    "        df_unified.loc[df_unified['jurisdiction'].isin(['Federal FLSA', 'FEDERAL']), 'jurisdiction'] = 'Federal'\n",
    "        \n",
    "        return df_unified\n",
    "        \n",
    "        \n",
    "        \n",
    "    def collect_all_footnotes(self, df_standard, df_tipped) -> dict:\n",
    "        \"\"\"Coleta todos os footnotes de todos os datasets\"\"\"\n",
    "        \n",
    "        all_footnotes = {}\n",
    "        if hasattr(self, 'standard_footnotes'):\n",
    "            all_footnotes[1]=(self.standard_footnotes)\n",
    "        \n",
    "        # Tipped wage footnotes (do scraper)\n",
    "        if hasattr(self, 'tipped_footnotes'):\n",
    "            all_footnotes.update(self.tipped_footnotes)\n",
    "        \n",
    "        # Youth employment footnotes (se houver)\n",
    "        if hasattr(self, 'youth_footnotes'):\n",
    "            all_footnotes.update(self.youth_footnotes)\n",
    "        \n",
    "        return all_footnotes\n",
    "    \n",
    "    def transform(self, standard_footnotes: dict = None, tipped_footnotes: dict = None):\n",
    "        \"\"\"Executa o pipeline completo de transforma√ß√£o\"\"\"\n",
    "        \n",
    "        # Guardar footnotes para usar depois\n",
    "        self.standard_footnotes = standard_footnotes or {}\n",
    "        self.tipped_footnotes = tipped_footnotes or {}\n",
    "        \n",
    "        # 1. Transformar datasets\n",
    "        df_standard_transformed = self.transform_standard_to_long()\n",
    "        df_tipped_transformed = self.transform_tipped_to_long()\n",
    "        \n",
    "        # 2. Garantir colunas comuns\n",
    "        common_columns = [\n",
    "            'jurisdiction', 'year', 'category_name', 'category_type',\n",
    "            'base_wage_per_hour', 'minimum_cash_wage', 'maximum_tip_credit',\n",
    "            'frequency', 'notes', 'source_url', 'footnotes' \n",
    "        ]\n",
    "        \n",
    "        for col in common_columns:\n",
    "            if col not in df_standard_transformed.columns:\n",
    "                df_standard_transformed[col] = None\n",
    "            if col not in df_tipped_transformed.columns:\n",
    "                df_tipped_transformed[col] = None\n",
    "        \n",
    "        df_unified = pd.concat([\n",
    "            df_standard_transformed[common_columns],\n",
    "            df_tipped_transformed[common_columns]\n",
    "        ], ignore_index=True)\n",
    "\n",
    "        df_unified = self.default_state_name(df_unified=df_unified)\n",
    "        # 4. Criar Dim_Footnotes (antes de criar fato)\n",
    "        all_footnotes = self.collect_all_footnotes(df_standard_transformed, df_tipped_transformed)\n",
    "        print(all_footnotes)\n",
    "        self.create_dim_footnotes_unified(all_footnotes)\n",
    "        \n",
    "        # 5. Criar dimens√µes\n",
    "        self.create_dim_states(df_unified)\n",
    "        \n",
    "        self.create_dim_categories(df_unified)\n",
    "        \n",
    "        # 6. Criar fato\n",
    "        self.create_fact_table(df_unified)\n",
    "        \n",
    "        # 7. Criar bridge\n",
    "        self.create_bridge_table(all_footnotes)\n",
    "        \n",
    "        return {\n",
    "            'fact': self.fact_minimum_wage,\n",
    "            'dim_states': self.dim_states,\n",
    "            'dim_categories': self.dim_categories,\n",
    "            'dim_footnotes': self.dim_footnotes,\n",
    "            'bridge': self.bridge_wage_footnote\n",
    "        }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fun√ß√£o principal para teste\"\"\"\n",
    "    # Criar dados de exemplo]\n",
    "    scraper_standard = MinimumWageScraper()\n",
    "    df_standard_raw = scraper_standard.scrape()\n",
    "    display(df_standard_raw)\n",
    "    processor_standard = StandardWageProcessor(df_standard_raw, scraper_standard.footnotes_dict)\n",
    "    df_standard_processed = processor_standard.process()\n",
    "    \n",
    "    scraper_tipped = TippedWageScraper()\n",
    "    df_tipped_raw = scraper_tipped.scrape(\n",
    "            start_year=2003,\n",
    "            end_year=2025\n",
    "        )\n",
    "\n",
    "    \n",
    "        # 2. Processar tipped wages\n",
    "    processor_tipped = TippedWageProcessor(df_tipped_raw, scraper_tipped.footnotes_dict)\n",
    "    df_tipped_processed = processor_tipped.process()\n",
    "  \n",
    "    transformer = DataTransformer(df_standard_processed, df_tipped_processed)\n",
    "    result = transformer.transform(\n",
    "        standard_footnotes=processor_standard.footnotes_dict,\n",
    "        tipped_footnotes=processor_tipped.footnotes_dict\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìã Preview das tabelas:\")\n",
    "    for name, df in result.items():\n",
    "        print(f\"\\n{name.upper()}:\")\n",
    "        print(df.head())\n",
    "    \n",
    "    return result\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".proj_cdia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
