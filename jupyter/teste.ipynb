{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ed7ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/projeto_adp/.pj_mwgt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "import tiktoken\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "464e0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://www.dol.gov'\n",
    "\n",
    "load_dotenv()\n",
    " \n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0bb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    \"\"\"Fetch HTML content from a given URL.\"\"\"\n",
    "    response = req.get(url)\n",
    "    response.raise_for_status()  # Raise an error for bad responses\n",
    "    return response.text\n",
    "\n",
    "def search_for_link(html):\n",
    "    \"\"\"Parse HTML and extract relevant data.\"\"\"\n",
    "    data = []\n",
    "    for link in html.find_all('a', href=True):\n",
    "        data.append({\n",
    "            'text': link.get_text(strip=True),\n",
    "            'url': BASE_URL + link['href'] if link['href'].startswith('/') else link['href']\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "def adjust_text(soup, link):\n",
    "    \"\"\"Substitui footnotes <a href=\"#...\"> pelo texto do rodapé sem o número.\"\"\"\n",
    "    if not link:\n",
    "        return soup.get_text(\" \", strip=True)\n",
    "\n",
    "    url = link[0]['url']\n",
    "    href_html = get_html(url)\n",
    "    href_soup = BeautifulSoup(href_html, \"html.parser\")\n",
    "\n",
    "    if '#' in url:\n",
    "        footnote_id = url.split('#')[-1]\n",
    "        div = href_soup.find('div', {'id': footnote_id})\n",
    "        if div:\n",
    "            for sup in div.find_all(\"sup\"):\n",
    "                sup.decompose()\n",
    "            footnote_text = div.get_text(strip=True)\n",
    "\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                if footnote_id in a[\"href\"]:\n",
    "                    a.replace_with(\" \" + footnote_text + \" \")\n",
    "\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normaliza caracteres, remove espaços extras e caracteres de controle.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = \"\".join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c116b44",
   "metadata": {},
   "source": [
    "__Get HTML__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f691ad2",
   "metadata": {},
   "source": [
    "## Check if Guam, Northen Mariana Island, Puerto Rico, Virgin Island, America Samoa need to be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fba1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = get_html(f\"{BASE_URL}/agencies/whd/minimum-wage/state\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "cat = soup.find('div', {\"id\": \"states\"})\n",
    "state = cat.find_all('div')\n",
    "\n",
    "docs = {}\n",
    "\n",
    "for state_name in state:\n",
    "    if state_name.get('id').lower() == 'as': # America Samoa is a special case\n",
    "        continue\n",
    "    link = search_for_link(state_name)\n",
    "\n",
    "    # pega os irmãos após o <h2> (já como soup)\n",
    "    siblings = state_name.h2.find_next_siblings()\n",
    "    temp_soup = BeautifulSoup(\"\".join(str(s) for s in siblings), \"html.parser\")\n",
    "\n",
    "    # 1. Substitui footnotes\n",
    "    clean_text = adjust_text(temp_soup, link)\n",
    "\n",
    "    # 2. Normaliza texto\n",
    "    norm_text = normalize_text(clean_text)\n",
    "    norm_text = f\"For the state of the {state_name.h2.text} the laws of mimum wage is: \" + norm_text\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    docs[doc_id] = {\"text\": norm_text, \"metadata\": {\"state\": state_name.h2.text, 'text':norm_text}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff510355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f9dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"PINECONE_API\")\n",
    "pc = Pinecone(api_key=api_key)\n",
    "index_name = 'firtsindex'\n",
    "indexes = pc.list_indexes()\n",
    "index_names = [index.name for index in indexes]\n",
    "if index_name not in index_names:\n",
    "    pc.create_index(name=index_name,\n",
    "                dimension=384,\n",
    "                spec=ServerlessSpec(cloud='aws',region='us-east-1'),\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc72060",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index('firstindex')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9dd01d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_pinecone(doc, model, index):\n",
    "    for id, state in doc.items():\n",
    "        embeddings = model.encode(state['text']).tolist()\n",
    "\n",
    "        pinecone_data = {\n",
    "            \"id\": id,\n",
    "            \"values\": embeddings,\n",
    "            \"metadata\": state[\"metadata\"]\n",
    "        }\n",
    "\n",
    "        index.upsert(vectors=[pinecone_data])\n",
    "add_to_pinecone(docs,model, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb84d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(index, query, n_results):\n",
    "    query_embedding = model.encode([query]).tolist()[0]\n",
    "    results = index.query(vector=query_embedding,\n",
    "                          top_k=n_results,\n",
    "                          include_metadata=True)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81f5f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "groq_api_key = os.getenv(\"GROQ_API\")\n",
    "client_groq = Groq(api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f20437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_context(results):\n",
    "    return \"\\n\\n\".join([match['metadata']['text'] for match in results['matches']])\n",
    "\n",
    "def get_prompt(context: str, query: str):\n",
    "    prompt = f\"\"\"Based on the following context and conversation history, \n",
    "        please provide a relevant and contextual response. \n",
    "        If the answer cannot be derived from the context, only use the conversation history \n",
    "        or say \"I cannot answer this based on the provided information.\"\n",
    "\n",
    "        Context from documents:\n",
    "        {context}\n",
    "\n",
    "        Human: {query}\n",
    "\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_response(query: str, context: str):\n",
    "    \"\"\"Generate a response using Groq's Llama-3.1-8b-instant with conversation history\"\"\"\n",
    "    # Construct the prompt\n",
    "    prompt = get_prompt(context, query)\n",
    "\n",
    "    try:\n",
    "        # Create the chat completion request\n",
    "        completion = client_groq.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about the minimal wage on USA\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_tokens=1024,\n",
    "            top_p=1,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "\n",
    "        # Process and return the streamed response\n",
    "        response_content = []\n",
    "        for chunk in completion:\n",
    "            content = chunk.choices[0].delta.content or \"\"\n",
    "            response_content.append(content)\n",
    "\n",
    "        # Combine the response into a single string\n",
    "        response = \"\".join(response_content)\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f1a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(pinecone_index, query: str, n_chunks: int = 2):\n",
    "    \"\"\"Perform RAG query: retrieve relevant chunks and generate an answer using Pinecone.\"\"\"\n",
    "    # Retrieve relevant chunks using Pinecone\n",
    "    results = semantic_search(pinecone_index, query, n_chunks)\n",
    "    context = get_context(results)\n",
    "\n",
    "    # Generate response using Groq\n",
    "    response = generate_response(query, context)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6c5c197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta:  What is the California minimal wage? Have any extra points? Bullet points\n",
      "Reposta:  The basic minimum wage in California is $16.50 per hour. Additionally, there are the following premium pay rules for overtime hours worked:\n",
      "\n",
      "* Daily:\n",
      "  • Any work in excess of 8 hours in a workday is paid at 1.5 times the regular rate of pay (time and a half)\n",
      "  • Any work in excess of 12 hours in a day is paid at double the regular rate of pay\n",
      "* Weekly:\n",
      "  • Any work in excess of 40 hours in a week is paid at 1.5 times the regular rate of pay (time and a half)\n",
      "  • The first 8 hours worked on the seventh day of a workweek are paid at 1.5 times the regular rate of pay (time and a half)\n",
      "  • Any work in excess of 8 hours on the seventh day of a workweek is paid at double the regular rate of pay\n"
     ]
    }
   ],
   "source": [
    "query = 'What is the California minimal wage? Have any extra points? Bullet points'\n",
    "response = rag_query(index, query)\n",
    "\n",
    "print('Pergunta: ', query)\n",
    "\n",
    "print('Reposta: ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feda743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'data/vdb_entities.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'data/vdb_relationships.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'data/vdb_chunks.json'} 0 data\n",
      "Rerank is enabled but no rerank_model_func provided. Reranking will be skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai._base_client:Retrying request to /embeddings in 0.393398 seconds\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Quick check to confirm\n",
    "print(\"OPENAI_API_KEY loaded:\", os.getenv(\"OPENAI_API_KEY\") is not None)\n",
    "\n",
    "# --- LightRAG setup ---\n",
    "from lightrag import LightRAG\n",
    "from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "\n",
    "rag = LightRAG(\n",
    "    working_dir=\"data/\",\n",
    "    embedding_func=openai_embed,\n",
    "    llm_model_func=gpt_4o_mini_complete,\n",
    ")\n",
    "\n",
    "# Initialize storages\n",
    "await rag.initialize_storages()\n",
    "await initialize_pipeline_status()\n",
    "\n",
    "# # Insert your docs\n",
    "for id, state in docs.items():\n",
    "    await rag.ainsert(state['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3bf5c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rerank is enabled but no rerank model is configured. Please set up a rerank model or set enable_rerank=False in query parameters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"### Overview of Minimum Wage Rates\\n\\nThe highest minimum wage in the United States is currently set in the **District of Columbia**, where it stands at **$17.95** per hour. This rate reflects the jurisdiction's commitment to ensuring fair compensation for workers.\\n\\n### State Comparisons\\n\\nFollowing the District of Columbia, **California** has one of the highest state minimum wages at **$16.50** per hour, joined closely by **Washington** at **$16.66** per hour. **Connecticut** also maintains a significant minimum wage at **$16.35** per hour, while **Oregon** sets its standard rate at **$15.05**, with higher rates for specific areas like the Portland Metro Area at **$16.30** per hour.\\n\\n### Additional Context\\n\\nDifferent states have varying rates influenced by local economic conditions and labor policies. As such, the minimum wage shows significant diversity across the country, emphasizing the ongoing efforts to adjust for economic changes and living costs.\\n\\n### References\\n- [KG] Unknown source\\n- [KG] Unknown source\\n- [KG] Unknown source\\n- [KG] Unknown source\\n- [KG] Unknown source \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightrag import QueryParam\n",
    "result = await rag.aquery(\"What is the biggest minimal wage?\", param=QueryParam(mode=\"mix\"))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4170de8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No tables found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m         table = get_table(url = \u001b[33m'\u001b[39m\u001b[33mhttps://www.dol.gov/agencies/whd/state/child-labor/door-to-door-sales/2023\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m     39\u001b[39m         display(table)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mextract_child_non_farm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mextract_child_non_farm\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_child_non_farm\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         table = \u001b[43mget_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://www.dol.gov/agencies/whd/state/child-labor/door-to-door-sales/2023\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     39\u001b[39m         display(table)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mget_table\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_table\u001b[39m(url : \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         tables = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tables\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projeto_adp/.pj_mwgt/lib/python3.11/site-packages/pandas/io/html.py:1240\u001b[39m, in \u001b[36mread_html\u001b[39m\u001b[34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1225\u001b[39m     [\n\u001b[32m   1226\u001b[39m         is_file_like(io),\n\u001b[32m   (...)\u001b[39m\u001b[32m   1230\u001b[39m     ]\n\u001b[32m   1231\u001b[39m ):\n\u001b[32m   1232\u001b[39m     warnings.warn(\n\u001b[32m   1233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal html to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_html\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1234\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projeto_adp/.pj_mwgt/lib/python3.11/site-packages/pandas/io/html.py:1003\u001b[39m, in \u001b[36m_parse\u001b[39m\u001b[34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m retained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m retained\n\u001b[32m   1005\u001b[39m ret = []\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projeto_adp/.pj_mwgt/lib/python3.11/site-packages/pandas/io/html.py:983\u001b[39m, in \u001b[36m_parse\u001b[39m\u001b[34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[39m\n\u001b[32m    972\u001b[39m p = parser(\n\u001b[32m    973\u001b[39m     io,\n\u001b[32m    974\u001b[39m     compiled_match,\n\u001b[32m   (...)\u001b[39m\u001b[32m    979\u001b[39m     storage_options,\n\u001b[32m    980\u001b[39m )\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     tables = \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[32m    985\u001b[39m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[32m    986\u001b[39m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[33m\"\u001b[39m\u001b[33mseekable\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io.seekable():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projeto_adp/.pj_mwgt/lib/python3.11/site-packages/pandas/io/html.py:249\u001b[39m, in \u001b[36m_HtmlFrameParser.parse_tables\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    242\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[32m    244\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m \u001b[33;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     tables = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projeto_adp/.pj_mwgt/lib/python3.11/site-packages/pandas/io/html.py:598\u001b[39m, in \u001b[36m_BeautifulSoupHtml5LibFrameParser._parse_tables\u001b[39m\u001b[34m(self, document, match, attrs)\u001b[39m\n\u001b[32m    596\u001b[39m tables = document.find_all(element_name, attrs=attrs)\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tables:\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo tables found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    600\u001b[39m result = []\n\u001b[32m    601\u001b[39m unique_tables = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: No tables found"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def get_table(url : str):\n",
    "        tables = pd.read_html(url)\n",
    "        return tables\n",
    "    \n",
    "def get_footnotes( url: str, div: str = \"content\"):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    content = soup.find(\"div\", id=div)\n",
    "    if not content:\n",
    "        return []\n",
    "\n",
    "    table = content.find(\"table\")\n",
    "\n",
    "    if table:\n",
    "        footnote_elements = []\n",
    "        for sibling in table.find_next_siblings():\n",
    "            # Para evitar ruído (espaços em branco, '\\n', etc.)\n",
    "            if getattr(sibling, \"name\", None):\n",
    "                footnote_elements.append(sibling)\n",
    "        return footnote_elements\n",
    "\n",
    "    elements = list(content.children)\n",
    "    start_index = None\n",
    "    for i, el in enumerate(elements):\n",
    "        if hasattr(el, \"get_text\") and \"FOOTNOTE\" in el.get_text().upper():\n",
    "            start_index = i\n",
    "            break\n",
    "\n",
    "    if start_index is not None:\n",
    "        return elements[start_index + 1 :]\n",
    "    else:\n",
    "        return []\n",
    "def extract_child_non_farm():\n",
    "        table = get_table(url = 'https://www.dol.gov/agencies/whd/state/child-labor/door-to-door-sales/2023')[0]\n",
    "        \n",
    "        display(table)\n",
    "extract_child_non_farm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pj_mwgt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
